\documentclass{article}
\usepackage[a4paper, left=3cm, right=3cm, top=2.5cm, bottom=2.5cm]{geometry}
\usepackage{multicol} % <-- پکیج اضافه شده برای لیست‌های چند ستونی

\usepackage{xcolor}
\usepackage{listings}



% یک استایل واحد برای تمام کدها
\lstset{
    basicstyle=\ttfamily\footnotesize,
    backgroundcolor=\color{gray!5},
    frame=single,
    numbers=left,
    numberstyle=\tiny\color{gray},
    numbersep=5pt,
    tabsize=4,
    captionpos=b,
    breaklines=true,
    breakatwhitespace=true,
    showstringspaces=false,
    extendedchars=true,
    inputencoding=utf8,
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red},
    keepspaces=true
}


\usepackage{xepersian}
\settextfont{XB Niloofar}
\setlatintextfont{Times New Roman}

% ***************************************************************************************************


\begin{document}

\title{عنوان پروژه: \\
\begin{LTR}
Multimodal RAG
\end{LTR}}
\author{
نام دانشجو: حسین آریان‌مهر\\
شماره دانشجویی: 4011262167\\
استاد: دکتر هادی صدوقی یزدی
}
\date{}
\maketitle


% **************************************************************************************************


\newpage
\section{فهرست}

% استفاده از فهرست خودکار به جای لیست دستی
\tableofcontents
\newpage


% **************************************************************************************************


\section{مقدمه}
در سال‌های اخیر، مدل‌های زبانی بزرگ (\lr{LLMs}) توانسته‌اند پیشرفت چشم‌گیری در درک و تولید زبان طبیعی داشته باشند.
با این حال، این مدل‌ها محدود به دانشی هستند که در زمان آموزش به آن‌ها داده شده است و توانایی دسترسی مستقیم به داده‌های جدید یا به‌روزشده را ندارند.
برای حل این محدودیت، رویکردی به نام \lr{RAG (Retrieval-Augmented Generation)} معرفی شده است.
\lr{RAG} ترکیبی از دو بخش اصلی است:
\begin{enumerate}
\item بازیابی اطلاعات (\lr{Retrieval}) از منابع بیرونی مانند پایگاه‌های داده، اسناد متنی یا بردارهای معنایی،
\item تولید پاسخ (\lr{Generation}) با استفاده از مدل زبانی که از اطلاعات بازیابی‌شده به‌صورت زمینه‌ای بهره می‌گیرد.
\end{enumerate}

به بیان ساده، \lr{RAG} به جای اتکا صرف به حافظه‌ی داخلی مدل، از داده‌های واقعی و به‌روز برای تولید پاسخ دقیق‌تر، مرتبط‌تر و قابل اعتمادتر استفاده می‌کند.
این روش باعث می‌شود مدل در پاسخ‌گویی به سؤالات تخصصی، استناد به منابع و کاهش خطاهای توهم‌زا (\lr{hallucinations}) عملکرد بهتری داشته باشد.
با این حال، در بسیاری از کاربردهای واقعی، داده‌های موجود فقط متنی نیستند.
امروزه حجم زیادی از اطلاعات به‌صورت تصویر، صوت، ویدیو و سایر حالت‌های غیرمتنی ذخیره می‌شوند.
اینجاست که مفهوم \lr{Multimodal RAG} مطرح می‌شود.

\lr{Multimodal RAG} نسخه‌ی پیشرفته‌تری از \lr{RAG} است که قادر است از چند نوع داده (یا «مودالیتی») به‌صورت هم‌زمان استفاده کند؛
\lr{Multimodal RAG (Retrieval-Augmented Generation)} ترکیبی از دو مفهوم کلیدی در هوش مصنوعی است: بازیابی اطلاعات (\lr{Retrieval}) و تولید متن (\lr{Generation})، که در آن مدل نه تنها از داده‌های متنی بلکه از چندین نوع داده (مانند تصویر، صدا، و ویدیو) برای پاسخ‌گویی استفاده می‌کند.
در این معماری، ابتدا ورودی کاربر که می‌تواند شامل متن، تصویر یا صوت باشد، پردازش می‌شود.
برای هر نوع داده، بردار ویژگی (\lr{Embedding}) تولید می‌شود تا بتوان آن را در فضای مشترک مقایسه کرد.
سپس این بردارها به یک پایگاه داده برداری (\lr{Vector Database}) مانند \lr{Weaviate}، \lr{Pinecone} یا \lr{FAISS} فرستاده می‌شوند تا نزدیک‌ترین داده‌ها (\lr{retrieved items}) از میان داده‌های ذخیره‌شده پیدا شوند.
این داده‌های بازیابی‌شده که ممکن است شامل چندین مدالیته (مثل توضیح متنی، تصویر مرتبط، یا توصیف صوتی) باشند، به‌صورت ساختاریافته به یک مدل زبانی بزرگ (\lr{LLM}) فرستاده می‌شوند.
مدل زبانی با ترکیب دانش خود و داده‌های بازیابی‌شده، پاسخ نهایی را تولید می‌کند.
ویژگی اصلی \lr{Multimodal RAG} این است که برخلاف \lr{RAG} سنتی که فقط با متن کار می‌کند، می‌تواند اطلاعات متنوع را باهم ترکیب کند؛
مثلاً از روی تصویر و توضیحات صوتی درباره آن نتیجه‌گیری کند.
این سیستم کاربردهای گسترده‌ای دارد، از جمله در چت‌بات‌های هوشمند چندوجهی، جست‌وجوی تصویری و صوتی، سیستم‌های آموزشی تعاملی، و تحلیل داده‌های چندرسانه‌ای.
نیاز به \lr{Multimodal RAG} از آن‌جا ناشی می‌شود که بسیاری از مسائل دنیای واقعی چندوجهی هستند.
برای مثال، در یک سامانه‌ی جست‌وجوی هوشمند ممکن است کاربر تصویری از یک شیء ارسال کند، توضیح صوتی ارائه دهد، یا سؤالی متنی بپرسد، و سیستم باید بتواند از میان انواع داده‌های موجود، مرتبط‌ترین پاسخ را تولید کند.
به طور خلاصه، \lr{Multimodal RAG} با ترکیب قابلیت‌های درک چندنوع داده و تولید پاسخ زبانی، مشکلات زیر را برطرف می‌کند:
\begin{itemize}
\item محدودیت مدل‌های متنی در استفاده از داده‌های تصویری یا صوتی،
\item ضعف مدل‌های زبانی در به‌روزرسانی دانش و استناد به داده‌های جدید،
\item ناتوانی سیستم‌های تک‌موداله در پاسخ به پرسش‌های پیچیده و چندمنظوره.
\end{itemize}

بنابراین، \lr{Multimodal RAG} گامی مهم در جهت ایجاد سامانه‌های هوشمند جامع‌تر، تعاملی‌تر و آگاه‌تر از دنیای واقعی محسوب می‌شود.

\subsection{کاربردهای \lr{Multimodal RAG}}
% استفاده از محیط چند ستونی برای لیست طولانی
\begin{multicols}{2}
\begin{itemize}
\item جست‌وجوی هوشمند چندرسانه‌ای
\item سیستم‌های پاسخ‌گوی تصویری و صوتی
\item دستیارهای مجازی با ورودی چندحالته
\item تحلیل محتوای چندرسانه‌ای (متن، تصویر، صوت)
\item بازیابی اطلاعات پزشکی از تصاویر و گزارش‌ها
\item سیستم‌های آموزشی تعاملی
\item تحلیل و مدیریت داده‌های امنیتی (تصویر و صدا)
\item تولید توضیحات خودکار برای تصاویر یا ویدیوها
\item موتورهای توصیه‌گر چندرسانه‌ای
\item ربات‌های گفتگوگر با درک چندوجهی
\end{itemize}
\end{multicols}


% *********************************************************************************************


\section{انواع روش‌های \lr{Multimodal RAG}}

% حذف شماره‌گذاری دستی از عنوان
\subsection{رویکرد مبتنی بر توصیف متنی \lr{(Textual Description-Based MRAG)}}

در این روش، برای هر داده‌ی غیرمتنی مانند تصویر یا ویدئو، یک توصیف متنی (\lr{Textual Description}) توسط یک مدل زبانی بزرگ (مانند \lr{Gemini} یا \lr{GPT}) تولید می‌شود.
به این ترتیب، تمامی داده‌ها — صرف‌نظر از نوع \lr{modality} — در قالب متن بازنمایی می‌شوند.
در مرحله‌ی بازیابی (\lr{Retrieval})، ورودی متنی کاربر با توصیف‌های متنی موجود در پایگاه داده‌ی برداری (\lr{Vector Database}) مقایسه می‌شود و داده‌هایی که از لحاظ معنایی بیشترین شباهت را دارند
(اعم از متن یا تصویر و...) بازیابی می‌گردند.
در نهایت، داده‌های بازیابی‌شده به همراه ورودی کاربر به مدل زبانی ارسال می‌شوند تا خروجی نهایی تولید شود.

% استفاده از description list برای مزایا و معایب
\subsubsection{مزایا}
\begin{description}
\item[پیاده‌سازی ساده] امکان پیاده‌سازی \lr{MRAG} حتی در شرایطی که مدل زبانی چندوجهی (\lr{MLLM}) در دسترس نباشد.

\item[هزینه کمتر] استفاده از مدل‌های تولید توضیح (\lr{Captioning / Translation Models}) که سبک‌تر و ارزان‌تر از مدل‌های \lr{MLLM} هستند.

\end{description}

\subsubsection{معایب}
\begin{description}
\item[افت اطلاعات] احتمال بروز افت اطلاعات (\lr{Information Loss}) در فرآیند تبدیل داده‌های غیرمتنی به متن.

\item[دقت پایین‌تر] دقت پایین‌تر در مقایسه با روش‌های چندوجهی مستقیم، به‌ویژه در کاربردهایی که جزئیات بصری یا صوتی اهمیت بالایی دارند.

\end{description}

\subsubsection{کاربرد پیشنهادی}
این روش زمانی مناسب است که مدل زبانی فقط از داده‌های متنی پشتیبانی کند (\lr{LLM} غیرچندوجهی).
با این حال، حتی در مدل‌های چندوجهی نیز می‌توان از این رویکرد برای انجام مرحله‌ی \lr{Retrieval} بر اساس متن استفاده نمود،
و سپس داده‌های اصلی را برای مرحله‌ی تولید نهایی (\lr{Generation}) به \lr{MLLM} ارسال کرد.


% حذف شماره‌گذاری دستی
\subsection{رویکرد مبتنی بر فضای تعبیه‌ی مشترک \lr{(Unified Embedding Space MRAG)}}
در این روش، تمامی داده‌ها — اعم از متن، تصویر، صوت و ویدئو — به یک فضای تعبیه‌ی مشترک (\lr{Shared Embedding Space}) نگاشت می‌شوند.
برای این منظور از مدل‌هایی مانند \lr{CLIP} استفاده می‌شود که قادرند داده‌های متنی و تصویری را در یک فضا نمایش دهند.
در این حالت، ورودی کاربر نیز به همان فضای تعبیه نگاشت می‌شود و داده‌هایی که بیشترین شباهت را از نظر برداری دارند، بازیابی شده و به مدل زبانی ارسال می‌گردند.

\subsubsection{مزایا}
\begin{itemize}
\item دقت بالاتر نسبت به روش اول.
\item کاهش چشمگیر افت اطلاعات (\lr{Information Loss}) به دلیل عدم نیاز به توصیف متنی واسطه.

\item امکان سنجش مستقیم شباهت بین \lr{modality}های مختلف (برای مثال: تصویر و متن).

\end{itemize}

\subsubsection{چالش‌ها}
\begin{itemize}
\item نیاز به مدل‌های بسیار انعطاف‌پذیر که بتوانند تمامی \lr{modality}ها (متن، تصویر، صوت، ویدئو و...) را به یک فضای مشترک مپ کنند.

\item محدود بودن مدل‌هایی که چنین قابلیتی دارند (برای مثال \lr{CLIP} فقط برای تصویر و متن طراحی شده است).
\end{itemize}

% حذف شماره‌گذاری دستی
\subsection{رویکرد چندمدلی ترکیبی \lr{(Hybrid Multimodal Embedding MRAG)}}

در صورتی که مدلی وجود نداشته باشد که تمام \lr{modality}ها را به یک فضای تعبیه‌ی واحد نگاشت کند، می‌توان از مجموعه‌ای از مدل‌ها استفاده کرد که هر یک تنها بخشی از \lr{modality}ها را پوشش می‌دهند.
برای مثال:
\begin{itemize}
\item مدلی مانند \lr{CLIP} برای \textbf{متن و تصویر}
\item مدلی دیگر برای \textbf{صوت و متن}
\item و مدل‌های مشابه برای سایر ترکیبات
\end{itemize}

در زمان بازیابی، ورودی کاربر به همه‌ی این مدل‌ها داده می‌شود تا \lr{embedding} متناظر در هر فضای تعبیه تولید شود.
سپس، فرآیند بازیابی در هر فضای برداری انجام شده و مجموعه‌ای از داده‌های مشابه از هر مدل به‌دست می‌آید.
در نهایت، از یک \lr{Re-Ranker} برای انتخاب n مورد از نزدیک‌ترین داده‌ها به ورودی کاربر استفاده می‌شود و این داده‌ها به مدل زبانی چندوجهی (\lr{MLLM}) ارسال می‌گردند.

\subsubsection{مزایا}
\begin{description}
\item[انعطاف‌پذیری] انعطاف‌پذیری بالا در شرایطی که مدل یکپارچه‌ی چندوجهی در دسترس نیست.

\item[بهینه‌سازی] امکان استفاده‌ی هم‌زمان از مدل‌های بهینه‌شده برای \lr{modality}های خاص.

\end{description}

\subsubsection{معایب}
\begin{description}
\item[پیچیدگی] پیچیدگی پیاده‌سازی بالا به‌دلیل نیاز به هماهنگی بین چند فضای تعبیه‌ی متفاوت.

\item[نیاز محاسباتی] نیاز به زمان و توان محاسباتی بیشتر برای انجام فرآیندهای \lr{retrieve} و \lr{re-rank}.

\end{description}


% ************************************************************************************************


\section{پایگاه داده برداری}
پایگاه داده برداری سیستمی است که می‌تواند بردارهای عددی با ابعاد بالا (یعنی \lr{embedding}ها) را ذخیره، ایندکس و جست‌وجو کند.
این بردارها معمولاً نشان‌دهنده ویژگی‌های معنایی داده‌هایی مانند متن، تصویر، صوت یا ترکیبی از آن‌ها هستند.
وقتی کاربر یک پرسش وارد می‌کند، آن پرسش نیز به یک بردار تبدیل می‌شود و سپس پایگاه داده، بردارهای نزدیک از نظر فاصله یا شباهت را بازمی‌گرداند، یعنی مشابه‌ترین داده‌های ذخیره‌شده را پیدا می‌کند.
از آن‌جا که بردارها در ابعاد زیاد هستند، معمولاً از الگوریتم‌های میان‌گرا (\lr{Approximate Nearest Neighbor}) مانند \lr{HNSW} و روش‌های مشابه برای سرعت بخشیدن به جست‌وجو استفاده می‌شود.
\subsection{دلایل نیاز به پایگاه دادهٔ برداری و ناکافی بودن پایگاه داده‌های سنتی}
پایگاه دادهٔ برداری نوعی سامانهٔ ذخیره‌سازی و بازیابی داده است که برای مدیریت مؤثر داده‌های با ابعاد بالا طراحی شده است.
این نوع پایگاه داده‌ها نقش مهمی در کاربردهای مبتنی بر هوش مصنوعی، به‌ویژه در جستجوی معنایی و سیستم‌های \lr{RAG} دارند.
در ادامه، دلایل اصلی نیاز به چنین پایگاه داده‌هایی بیان می‌شود:

\begin{enumerate}
\item \textbf{جستجوی شباهت معنایی (\lr{Semantic Similarity Search})} \\
در بسیاری از کاربردها هدف یافتن محتواهای مرتبط از نظر معناست، نه صرفاً از نظر تطابق واژگانی.
در این حالت، هر داده (مانند متن یا تصویر) به یک بردار عددی در فضای ویژگی‌ها تبدیل می‌شود و جستجو بر اساس معیارهایی مانند شباهت کسینوسی یا فاصلهٔ اقلیدسی انجام می‌گیرد.

\item \textbf{کارایی و مقیاس‌پذیری در داده‌های بزرگ و غیرساختاریافته} \\
بخش زیادی از داده‌های دنیای واقعی (نظیر متن، تصویر و صوت) غیرساختاریافته‌اند.
مدل‌های یادگیری عمیق این داده‌ها را به بردار تبدیل می‌کنند، و پایگاه دادهٔ برداری امکان ذخیره، ایندکس‌گذاری و بازیابی سریع این بردارها را در مقیاس وسیع فراهم می‌کند.

\item \textbf{پشتیبانی از به‌روزرسانی پویا و داده‌های در حال تغییر} \\
در سیستم‌های تعاملی یا مبتنی بر یادگیری مداوم، داده‌ها به‌صورت پیوسته اضافه یا اصلاح می‌شوند.
پایگاه دادهٔ برداری از درج، حذف و به‌روزرسانی بردارها پشتیبانی می‌کند و ایندکس‌های خود را به شکل بهینه به‌روزرسانی می‌نماید.

\item \textbf{محدودیت پایگاه داده‌های سنتی در جستجوی معنایی} \\
پایگاه داده‌های رابطه‌ای یا \lr{NoSQL} برای داده‌های ساختاریافته طراحی شده‌اند و قابلیت جستجو در فضای معنایی را ندارند.
این پایگاه‌ها معمولاً تنها بر پایهٔ تطابق کلیدواژه عمل می‌کنند و نمی‌توانند مشابهت مفهومی میان داده‌ها را تشخیص دهند.

\end{enumerate}

به طور خلاصه، پایگاه داده‌های سنتی در ذخیره و جستجوی داده‌های برداری با ابعاد بالا ناکارآمد هستند، در حالی که پایگاه دادهٔ برداری به‌طور اختصاصی برای جستجوی شباهت‌محور، پردازش داده‌های غیرساختاریافته و به‌روزرسانی پویا طراحی شده است.

\subsection{چند نمونه از پایگاه داده‌های برداری \lr{(Vector Databases)}}
% استفاده از description list برای تعریف اصطلاحات
\begin{description}
\item[\lr{Pinecone}] یک سرویس ابری کاملاً مدیریت‌شده برای ذخیره و جستجوی بردارها است که کار با آن ساده بوده و برای پروژه‌های مقیاس‌پذیر و مبتنی بر هوش مصنوعی مناسب است.

\item[\lr{Milvus}] پایگاه داده‌ای منبع‌باز و بسیار مقیاس‌پذیر است که برای جستجوی شباهت در داده‌های حجیم (متن، تصویر، صوت) طراحی شده و در کاربردهای صنعتی استفاده می‌شود.

\item[\lr{Weaviate}] پایگاه دادهٔ منبع‌باز با قابلیت جستجوی ترکیبی است که هم برداری و هم کلیدواژه‌ای را پشتیبانی می‌کند و برای سیستم‌های \lr{RAG} و چت‌بات‌های هوشمند بسیار مناسب است.

\item[\lr{Chroma}] پایگاه داده‌ای سبک و ساده است که برای برنامه‌های مبتنی بر مدل‌های زبانی بزرگ (\lr{LLM}) طراحی شده و اغلب در پروژه‌های تحقیقاتی یا نمونه‌سازی سریع استفاده می‌شود.

\end{description}


% ***************************************************************************************************


\section{Weaviate}
\lr{Weaviate} یک پایگاه دادهٔ برداری متن‌باز است که به‌طور ویژه برای توسعهٔ سریع برنامه‌های مبتنی بر هوش مصنوعی طراحی شده است.
این سامانه قابلیت‌هایی مانند جستجوی برداری، جستجوی ترکیبی (\lr{Hybrid Search})، فیلترگذاری پیشرفته و ادغام مستقیم با مدل‌های یادگیری ماشین را در اختیار توسعه‌دهندگان قرار می‌دهد.
یکی از ویژگی‌های برجستهٔ \lr{Weaviate}، معماری \lr{AI-Native} آن است؛ به این معنا که از ابتدا برای کار با داده‌های برداری (\lr{Embeddings}) و عملیات مرتبط با هوش مصنوعی طراحی شده است، نه به عنوان افزونه‌ای بر یک پایگاه دادهٔ سنتی.
از مهم‌ترین مزایای \lr{Weaviate} می‌توان به موارد زیر اشاره کرد:
\begin{enumerate}
\item \textbf{جستجوی هیبرید (\lr{Hybrid Search}):} ترکیب جستجوی برداری و جستجوی متنی در یک چارچوب واحد برای دستیابی به نتایج دقیق‌تر.

\item \textbf{انعطاف‌پذیری در استقرار:} قابلیت اجرا به صورت محلی، در سرورهای شخصی یا به شکل سرویس مدیریت‌شده در فضای ابری.

\item \textbf{مدیریت داده‌های متادیتا و فیلترها:} امکان فیلتر کردن نتایج جستجو بر اساس ویژگی‌ها یا متادیتاهای دلخواه (مانند برچسب‌ها یا نوع داده).

\item \textbf{پشتیبانی از مدل‌های یادگیری ماشین:} ادغام آسان با مدل‌های \lr{embedding} و چارچوب‌های مختلف یادگیری ماشین.

\item \textbf{متن‌باز بودن:} استفاده از نسخهٔ \lr{self-hosted} بدون نیاز به پرداخت هزینهٔ لایسنس و با کنترل کامل بر زیرساخت و داده‌ها.

\end{enumerate}

\subsection{نسخه‌های Weaviate: لوکال و ابری}
\lr{Weaviate} در دو حالت اصلی قابل استفاده است: نسخهٔ لوکال (\lr{Self-hosted / On-premises}) و نسخهٔ ابری (\lr{Managed / Weaviate Cloud}).
هر کدام از این نسخه‌ها مزایا و کاربردهای خاص خود را دارند.

% حذف شماره‌گذاری دستی
\subsubsection{نسخهٔ لوکال (\lr{Self-hosted / On-premises})}

از آن‌جا که \lr{Weaviate} به‌صورت متن‌باز عرضه شده است، می‌توان آن را بر روی سرور یا سیستم محلی خود با استفاده از ابزارهایی مانند \lr{Docker} یا \lr{Kubernetes} راه‌اندازی کرد.
در این حالت، نیازی به پرداخت هزینهٔ لایسنس وجود ندارد و تنها هزینه‌ها مربوط به زیرساخت (نظیر سرور، فضای ذخیره‌سازی و نگهداری) خواهد بود.
در نسخهٔ لوکال، کنترل کامل بر داده‌ها، امنیت، پیکربندی، و بهینه‌سازی عملکرد بر عهدهٔ کاربر است.
عملکرد و مقیاس‌پذیری نیز مستقیماً به منابع سخت‌افزاری موجود بستگی دارد.

% حذف شماره‌گذاری دستی
\subsubsection{نسخهٔ ابری (\lr{Weaviate Cloud / Managed Service})}

\lr{Weaviate} نسخهٔ ابری خود را در قالب چند سطح خدمات ارائه می‌دهد:
% استفاده از description list
\begin{description}
\item[\lr{Serverless Cloud}]
این نسخه کاملاً مدیریت‌شده و دارای مقیاس خودکار است.
برای پروژه‌های توسعه، آزمایشی یا کاربردهای متوسط مناسب بوده و معمولاً دارای دورهٔ آزمایشی رایگان (\lr{Free Trial}) است.

\item[\lr{Enterprise Cloud}]
نسخه‌ای کاملاً مدیریت‌شده با منابع اختصاصی، پشتیبانی فنی و توافق‌نامهٔ سطح خدمات (\lr{SLA}) بالا.
مناسب برای پروژه‌های بزرگ و سازمان‌هایی است که به عملکرد و پایداری بالا نیاز دارند.

\item[\lr{Sandbox}]
نسخه‌ای کاملاً رایگان برای استفاده‌های کوتاه‌مدت و آزمایشی است.
داده‌های ذخیره‌شده در این نسخه تنها تا ۱۴ روز نگهداری می‌شوند و پس از آن به‌صورت خودکار از فضای ابری حذف خواهند شد.

\end{description}

\subsection{سرویس Weaviate و نحوهٔ راه‌اندازی نسخه‌های آن}
سرویس \lr{Weaviate} در دو نسخهٔ اصلی ارائه می‌شود: نسخهٔ لوکال (\lr{Self-Hosted}) و نسخهٔ ابری (\lr{Cloud}).
نسخهٔ لوکال به‌صورت متن‌باز (\lr{Open Source}) و رایگان در دسترس است و به کاربران این امکان را می‌دهد تا پایگاه دادهٔ خود را بر روی سرور یا سیستم محلی اجرا و مدیریت کنند.
برای راه‌اندازی نسخهٔ لوکال، می‌توان از ابزار \lr{Docker} استفاده کرد.
در این حالت، لازم است ابتدا \lr{Docker} روی سیستم نصب شود.
سپس فایلی با نام \lr{docker-compose.yml} ایجاد کرده و محتوای زیر در آن قرار گیرد:

\begin{latin}
\begin{lstlisting}
version: '3.4'
services:
  weaviate:
    image: semitechnologies/weaviate:latest
    restart: always
    ports:
      - "8080:8080"
      - "50051:50051"
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: "./data"
      DISABLE_MODULES: 'all'
\end{lstlisting}
\end{latin}

با اجرای دستور زیر، سرویس \lr{Weaviate} به صورت محلی بر روی پورت 8080 در دسترس خواهد بود:

\begin{latin}
\begin{lstlisting}
docker-compose up -d
\end{lstlisting}
\end{latin}

پس از راه‌اندازی، می‌توان با استفاده 
از کتابخانهٔ رسمی \lr{Weaviate} در پایتون، به سرور متصل شد و یک \lr{Collection} (مجموعه داده) ایجاد کرد.
نمونه‌کد زیر نحوهٔ انجام این کار را نشان می‌دهد:

\begin{latin}
\begin{lstlisting}
import weaviate
import weaviate.classes as wvc

# 1. اتصال به Weaviate محلی
try:
    client = weaviate.connect_to_local()
    print("Successfully connected to Weaviate!")
except Exception as e:
    print(f"Failed to connect to Weaviate: {e}")
    exit()

collection_name = "Multimodal_Collection"

# 2. حذف Collection قبلی (در صورت وجود)
if client.collections.exists(collection_name):
    print(f"Collection '{collection_name}' already exists. Deleting it.")
    client.collections.delete(collection_name)

# 3. ایجاد Collection جدید
print(f"Creating collection '{collection_name}'...")
my_collection = client.collections.create(
    name=collection_name,
    properties=[
        wvc.config.Property(name="modality", data_type=wvc.config.DataType.TEXT),
      
  wvc.config.Property(name="content", data_type=wvc.config.DataType.TEXT),
        wvc.config.Property(name="contentId", data_type=wvc.config.DataType.TEXT),
        wvc.config.Property(name="filePath", data_type=wvc.config.DataType.TEXT),
    ]
)
print(f"Collection '{collection_name}' created successfully.")

client.close()
\end{lstlisting}
\end{latin}

برای اتصال مجدد به پایگاه داده و استفاده از مجموعهٔ ایجادشده، می‌توان از کد زیر استفاده کرد:

\begin{latin}
\begin{lstlisting}
import weaviate

try:
    weaviate_client = weaviate.connect_to_local()
    print("✅ Connected to Weaviate")
except Exception as e:
    print(f"❌ Weaviate connection failed: {e}")
    weaviate_client = None
\end{lstlisting}
\end{latin}

این روش امکان استقرار سریع، کنترل کامل بر داده‌ها، و توسعهٔ پروژه‌های مبتنی بر هوش مصنوعی را در محیط محلی فراهم می‌کند.
\subsection{راه‌اندازی نسخهٔ Sandbox از Weaviate Cloud}
نسخهٔ \lr{Cloud} یا \lr{Weaviate Cloud Service (WCS)} به‌صورت تجاری ارائه می‌شود و استفاده از آن به‌طور کامل رایگان نیست.
با این حال، امکان استفادهٔ آزمایشی (\lr{Sandbox}) از این سرویس برای مدت محدود وجود دارد.
در نسخهٔ \lr{Sandbox}، می‌توان هر \lr{Collection} را به‌مدت ۱۴ روز به‌صورت رایگان در یک \lr{Cluster} در فضای ابری \lr{Weaviate} ایجاد و نگهداری کرد.
پس از پایان این دوره، داده‌ها به‌صورت خودکار حذف می‌شوند.
برای استفادهٔ بلندمدت یا در مقیاس بزرگ‌تر، لازم است از نسخه‌های پولی \lr{Weaviate Cloud} استفاده شود.
مراحل ایجاد یک نسخهٔ \lr{Sandbox} به شرح زیر است:
\begin{enumerate}
\item ورود به وب‌سایت رسمی \lr{Weaviate Cloud} به آدرس \lr{https://console.weaviate.cloud/}
\item ایجاد حساب کاربری جدید (\lr{Sign Up})
\item مراجعه به تب \lr{Clusters}
\item کلیک بر روی آیکون "+" برای ایجاد یک کلاستر جدید
\item انتخاب گزینهٔ \lr{Sandbox}
\item وارد کردن نام دلخواه برای پایگاه داده و انتخاب دکمهٔ \lr{Create Cluster}
\end{enumerate}

پس از ایجاد کلاستر، لازم است یک \lr{API Key} تولید کنید.
این کلید از طریق صفحهٔ تنظیمات همان کلاستر قابل دسترسی است.
سپس با استفاده از گزینهٔ ``\lr{How to Connect}'' می‌توانید آدرس‌های اتصال (\lr{URLs}) و دستورالعمل‌های مربوط به نحوهٔ ارتباط با پایگاه داده را مشاهده کرده و در کد خود مورد استفاده قرار دهید.
% ************************************************************************************************


\section{مدل‌های تولید \lr{Embedding}}
مدل‌های تولید \lr{Embedding}، مدل‌هایی هستند که داده‌های ورودی — مانند متن، تصویر، صوت یا حتی ویدیو — را به بردارهای عددی (\lr{vector representations}) تبدیل می‌کنند.
این بردارها به‌گونه‌ای طراحی می‌شوند که ویژگی‌های معنایی و محتوایی داده را در یک فضای ریاضی (\lr{latent space}) نمایش دهند.
در این فضا، داده‌های مشابه از نظر معنا یا محتوا به یکدیگر نزدیک‌تر هستند.
هدف اصلی این مدل‌ها ایجاد یک فضای معنایی مشترک میان داده‌های مختلف است؛
به‌طوری که بتوان روابط میان مدالیته‌های گوناگون (مانند شباهت بین متن و تصویر) را محاسبه کرد.
برای مثال، اگر مدل بداند «یک گربه روی میز» چه معنایی دارد، تصویر واقعی گربه روی میز نیز در همان فضای برداری نزدیک به این توصیف قرار می‌گیرد.
\subsection{کاربردهای مدل‌های \lr{Embedding}}
\begin{itemize}
\item جست‌وجوی چندوجهی (\lr{Multimodal Retrieval}): یافتن تصاویر، ویدیوها یا صداهایی که از نظر معنا با یک جمله یا پرسش متنی مشابه هستند.

\item سیستم‌های \lr{RAG} و \lr{Chatbot}های چندوجهی: ترکیب متن، تصویر و صوت در یک فضای مشترک برای پاسخ‌گویی هوشمندتر.

\item دسته‌بندی و خوشه‌بندی داده‌ها: بر اساس شباهت معنایی در فضای \lr{embedding}.

\item تشخیص شباهت تصویر یا متن (\lr{Similarity Matching}): برای سیستم‌های پیشنهاددهنده یا کشف محتوای مشابه.

\item درک زمینه‌ای در مدل‌های مولد: مثلاً برای تولید توضیح متن برای تصاویر (\lr{Image Captioning}).

\end{itemize}

\subsection{نمونه‌هایی از مدل‌های معروف تولید \lr{Embedding}}

% حذف شماره‌گذاری دستی و استفاده از description list
\subsubsection{\lr{CLIP (Contrastive Language–Image Pretraining)} — ساختهٔ \lr{OpenAI}}
\begin{description}
\item[قابلیت‌ها]
\begin{itemize}
\item آموزش‌دیده بر میلیون‌ها جفت متن و تصویر.

\item ایجاد فضای معنایی مشترک بین متن و تصویر.
\item امکان انجام جست‌وجوی متنی در میان تصاویر و بالعکس.

\end{itemize}
\item[مزایا] دقت بالا در درک ارتباط بین متن و تصویر؛ قابل استفاده برای \lr{zero-shot classification}.

\item[محدودیت‌ها] فقط از دو مدالیته (متن و تصویر) پشتیبانی می‌کند؛ عملکرد آن به کیفیت داده‌های آموزشی وابسته است.
\end{description}

% حذف شماره‌گذاری دستی و استفاده از description list
\subsubsection{\lr{ImageBind} — ساختهٔ \lr{Meta AI}}

\begin{description}
\item[قابلیت‌ها]
\begin{itemize}
\item پشتیبانی از شش مدالیته: تصویر، متن، صوت، حرکات (\lr{motion}), عمق (\lr{depth}), و داده‌های حرارتی (\lr{thermal}).

\item تمام این مدالیته‌ها را در یک فضای برداری واحد قرار می‌دهد.

\item امکان تطبیق داده‌های ناهمگون (مثلاً جست‌وجوی صوتی برای یافتن تصویر مرتبط).

\end{itemize}
\item[مزایا] ادغام چندنوع داده در یک فضای \lr{embedding} مشترک؛ مناسب برای پروژه‌های چندرسانه‌ای پیشرفته.

\item[محدودیت‌ها] مدل بزرگ و سنگین است؛ برای اجرا نیاز به \lr{GPU} قدرتمند دارد؛ داده‌های آموزشی آن عمومی نیستند.
\end{description}

% حذف شماره‌گذاری دستی و استفاده از description list
\subsubsection{\lr{OpenCLIP} — نسخهٔ متن‌باز \lr{CLIP}}

\begin{description}
\item[قابلیت‌ها] عملکرد مشابه \lr{CLIP} ولی با داده‌های عمومی و قابل دسترس.

\item[مزایا] قابل استفاده در پروژه‌های متن‌باز؛ قابل آموزش مجدد روی داده‌های اختصاصی.

\item[محدودیت‌ها] گاهی دقت پایین‌تر از \lr{CLIP} اصلی دارد؛ نیازمند تنظیم دقیق داده‌ها و پارامترهاست.
\end{description}

% حذف شماره‌گذاری دستی و استفاده از description list
\subsubsection{\lr{SigLIP (Google Research)}}

\begin{description}
\item[قابلیت‌ها] مدل بهینه‌شده بر پایهٔ \lr{CLIP} با بهبود در نحوهٔ آموزش تقابلی.

\item[مزایا] دقت بالاتر در ارزیابی شباهت تصویر–متن؛ پایداری بیشتر در داده‌های نویزی.

\item[محدودیت‌ها] فقط دو مدالیتهٔ متن و تصویر را پشتیبانی می‌کند.
\end{description}

% حذف شماره‌گذاری دستی و استفاده از description list
\subsubsection{\lr{Whisper Embeddings} (برای صوت) — ساختهٔ \lr{OpenAI}}

\begin{description}
\item[قابلیت‌ها] تبدیل گفتار به بردار معنایی یا متن توصیفی.

\item[کاربرد] جست‌وجوی معنایی میان فایل‌های صوتی، تحلیل گفتار، یا ترکیب با مدل‌های چندوجهی.

\item[محدودیت] تنها برای داده‌های صوتی طراحی شده است؛ نیاز به مدل مکمل برای ترکیب با متن یا تصویر دارد.

\end{description}

\subsection{جمع‌بندی}
مدل‌های تولید \lr{embedding} مانند \lr{CLIP} و \lr{ImageBind} ستون فقرات بسیاری از سیستم‌های \lr{Multimodal AI} هستند.
آن‌ها داده‌های متفاوت را در یک فضای عددی مشترک بازنمایی می‌کنند تا بتوان میان مدالیته‌های گوناگون ارتباط برقرار کرد.
\lr{CLIP} برای متن و تصویر بسیار کارآمد است، در حالی که \lr{ImageBind} یک گام جلوتر رفته و چندین نوع داده را در یک چارچوب یکپارچه ترکیب می‌کند.
انتخاب میان آن‌ها بسته به نیاز پروژه، نوع داده‌ها و منابع سخت‌افزاری در دسترس انجام می‌شود.
% ********************************************************************************************


\section{مدل \lr{OpenCLIP}}
مدل \lr{OpenCLIP} یکی از نسخه‌های متن‌باز و توسعه‌یافته مدل مشهور \lr{CLIP (Contrastive Language–Image Pretraining)} است که در اصل توسط شرکت \lr{OpenAI} معرفی شد.
این مدل به‌طور خاص برای درک و تحلیل ارتباط بین تصویر و متن طراحی شده و به‌عنوان یکی از ابزارهای کلیدی در سیستم‌های چندرسانه‌ای (\lr{Multimodal}) شناخته می‌شود.
کاربردهای آن شامل پروژه‌های \lr{RAG (Retrieval-Augmented Generation)}، جستجوی تصویری، تولید توضیحات خودکار برای تصاویر و سایر برنامه‌های هوش مصنوعی چندوجهی است.
\lr{OpenCLIP} به‌عنوان یک نسخهٔ متن‌باز (\lr{open-source}) توسط تیم‌های \lr{LAION} و \lr{Hugging Face} توسعه یافته است.
هدف از ایجاد این مدل، فراهم کردن نسخه‌ای آزاد و قابل استفاده برای عموم بود تا بتواند همان عملکرد \lr{CLIP} را بدون محدودیت‌های لایسنس \lr{OpenAI} ارائه دهد.
این مدل به‌صورت عمومی در مخازن \lr{GitHub} و از طریق کتابخانه‌هایی مانند \lr{open\_clip} و \lr{transformers} در دسترس است.
\lr{OpenCLIP} از مدل‌های ازپیش‌آموزش‌دیده مختلف (\lr{pretrained weights}) پشتیبانی می‌کند که بر روی مجموعه داده‌های بسیار بزرگ مانند \lr{LAION-400M} و \lr{LAION-2B} آموزش دیده‌اند.
هدف اصلی \lr{OpenCLIP} یادگیری ارتباط معنایی بین تصویر و متن است؛
به این معنا که مدل قادر است بفهمد کدام تصویر با کدام توضیح متنی بیشترین تطابق و نزدیکی معنایی را دارد.
این قابلیت، \lr{OpenCLIP} را به ابزار قدرتمندی برای تولید \lr{embedding} مشترک برای داده‌های تصویری و متنی تبدیل کرده است.
چنین \lr{embedding}هایی پایه و اساس بسیاری از سیستم‌های \lr{Multimodal RAG} و سایر برنامه‌های هوش مصنوعی پیشرفته هستند.
از مهم‌ترین کاربردهای \lr{OpenCLIP} می‌توان به موارد زیر اشاره کرد:
\begin{itemize}
\item جستجوی تصویر بر اساس متن (\lr{Text-to-Image Retrieval})
\item جستجوی متن بر اساس تصویر (\lr{Image-to-Text Retrieval})
\item ساخت \lr{embedding} مشترک برای داده‌های تصویری و متنی در سیستم‌های چندرسانه‌ای
\item استفاده به‌عنوان \lr{Vision Encoder} در مدل‌های بزرگ چندحالته مانند \lr{GPT-4V} یا \lr{LLaVA}
\end{itemize}

با توجه به متن‌باز بودن، \lr{OpenCLIP} امکان استفاده و آموزش مجدد روی داده‌های اختصاصی را فراهم می‌کند و به توسعه‌دهندگان اجازه می‌دهد از مزایای \lr{CLIP} در پروژه‌های شخصی یا تحقیقاتی بهره‌برداری کنند بدون آنکه محدود به لایسنس \lr{OpenAI} باشند.
این ویژگی‌ها \lr{OpenCLIP} را به یکی از گزینه‌های محبوب برای کاربردهای چندوجهی و هوش مصنوعی تعاملی تبدیل کرده است.
\subsection{نحوه عملکرد و استفاده از مدل \lr{CLIP / OpenCLIP}}
مدل \lr{OpenCLIP} از دو شبکهٔ عصبی اصلی تشکیل شده است: \lr{Text Encoder} و \lr{Image Encoder}.
\lr{Text Encoder} (رمزگذار متنی) معمولاً مبتنی بر مدل‌های \lr{Transformer} مانند \lr{BERT} یا نسخه‌های کوچک‌تر \lr{GPT} است.
وظیفهٔ این بخش، تبدیل متن ورودی — مانند جمله یا توضیح یک تصویر — به یک بردار عددی با ابعاد ثابت (\lr{embedding}) می‌باشد.
این بردار نمایانگر ویژگی‌های معنایی متن است و امکان مقایسه آن با سایر داده‌ها را فراهم می‌کند.
\lr{Image Encoder} (رمزگذار تصویری) معمولاً از مدل‌هایی مانند \lr{Vision Transformer (ViT)} یا \lr{ResNet} تشکیل شده است.
این بخش تصویر ورودی را به برداری عددی با همان ابعاد \lr{embedding} متنی تبدیل می‌کند تا بتوان آن را در همان فضای معنایی با متن مقایسه کرد.
در مرحلهٔ آموزش، مدل تلاش می‌کند که \lr{embedding} تصاویر و متن‌های مرتبط با آن‌ها را به یکدیگر نزدیک کند و \lr{embedding}‌های نامرتبط را از هم دور نگه دارد.
این فرایند تحت عنوان \lr{Contrastive Learning} (یادگیری تقابلی) شناخته می‌شود.
هدف آموزش معمولاً بر اساس شباهت کسینوسی (\lr{cosine similarity}) میان بردار تصویر و بردار متن تعریف می‌شود، به‌طوری که مدل سعی دارد تصاویر و متون مرتبط بیشترین شباهت کسینوسی را داشته باشند.

\subsubsection{نحوهٔ استفاده عملی از \lr{OpenCLIP}}
در کاربردهای عملی، \lr{OpenCLIP} به‌طور عمده برای استخراج \lr{embedding} از تصاویر و متون به کار می‌رود.
به‌عنوان مثال، وقتی کاربر یک تصویر ارسال می‌کند، \lr{Image Encoder} برداری ۵۱۲ یا ۱۰۲۴ بُعدی از تصویر تولید می‌کند.
هنگامی که یک توضیح متنی ارسال شود، \lr{Text Encoder} برداری با همان ابعاد ایجاد می‌کند.
با مقایسهٔ این بردارها از طریق معیارهایی مانند \lr{cosine similarity} می‌توان تعیین کرد که کدام تصویر با کدام متن بیشترین تطابق معنایی را دارد.
در سیستم‌های \lr{Multimodal RAG}، این \lr{embedding}ها در پایگاه‌های داده برداری مانند \lr{Weaviate} ذخیره می‌شوند تا در زمان پرس‌وجو، داده‌های مشابه و مرتبط سریع بازیابی شوند.

\subsection{مزایای \lr{OpenCLIP}}
\lr{OpenCLIP} مجموعه‌ای از ویژگی‌ها و مزایای عملی را ارائه می‌دهد:
\begin{itemize}
\item متن‌باز بودن: امکان استفاده در پروژه‌های تحقیقاتی و تجاری بدون محدودیت لایسنس.

\item پشتیبانی از مدل‌های متنوع: شامل \lr{ViT-B}، \lr{ViT-L}، \lr{ViT-H} و سایر نسخه‌ها که امکان انتخاب بین سرعت و دقت را فراهم می‌کنند.

\item سازگاری با پلتفرم‌های مدرن: قابلیت استفاده با \lr{PyTorch}، \lr{Hugging Face} و ادغام با پایگاه‌های داده برداری مانند \lr{Weaviate}.

\item عملکرد بالا در مقیاس بزرگ: امکان \lr{fine-tuning} روی داده‌های خاص دامنه برای بهبود عملکرد در کاربردهای تخصصی.

\item دسترسی به مدل‌های \lr{pretrained} گسترده: آموزش دیده بر روی مجموعه داده‌های عظیم و چندزبانه که دقت و انعطاف‌پذیری بالایی ارائه می‌دهند.

\end{itemize}

این ویژگی‌ها \lr{OpenCLIP} را به ابزاری قدرتمند و قابل اعتماد برای کاربردهای چندوجهی و تحلیل معنایی تصاویر و متن در پروژه‌های پیشرفته هوش مصنوعی تبدیل کرده است.

\subsection{استفاده از \lr{OpenCLIP} و بارگذاری وزن‌های ازپیش‌آموزش‌دیده}
برای بهره‌گیری از مدل \lr{OpenCLIP}، ابتدا باید وزن‌های ازپیش‌آموزش‌دیده (\lr{pretrained weights}) را از مخزن رسمی \lr{OpenCLIP} در \lr{GitHub} دانلود کنید.
این وزن‌ها شامل پارامترهای آموزش‌دیدهٔ مدل هستند و به شما امکان می‌دهند بدون نیاز به آموزش مجدد، مستقیماً از مدل برای استخراج ویژگی‌ها (\lr{feature extraction}) یا محاسبهٔ شباهت بین تصویر و متن استفاده کنید.
وزن‌های مدل‌ها از طریق لینک زیر در دسترس هستند:

\lr{OpenCLIP GitHub Repository}

پس از دانلود وزن‌ها، می‌توانید مدل را با استفاده از قطعه‌کد زیر بارگذاری کنید:

\begin{latin}
\begin{lstlisting}
import open_clip

clip_model, _, preprocess = open_clip.create_model_and_transforms(
    CLIP_MODEL_NAME, pretrained=PRETRAINED_LOCAL_PATH
)
tokenizer = open_clip.get_tokenizer(CLIP_MODEL_NAME)
clip_model.to(DEVICE)
clip_model.eval()
\end{lstlisting}
\end{latin}

\subsubsection{توضیحات کد}
\begin{itemize}
\item \lr{create\_model\_and\_transforms} مدل را ایجاد می‌کند و توابع پیش‌پردازش مناسب برای تصاویر را برمی‌گرداند.

\item \lr{get\_tokenizer} توکنایزر متن را بر اساس مدل انتخاب‌شده آماده می‌کند.

\item \lr{clip\_model.to(DEVICE)} مدل را به \lr{GPU} یا \lr{CPU} منتقل می‌کند.

\item \lr{clip\_model.eval()} مدل را در حالت ارزیابی (\lr{evaluation mode}) قرار می‌دهد تا آمادهٔ استفاده برای \lr{inference} شود.

\end{itemize}

پس از این مراحل، مدل آماده است تا ویژگی‌های متنی و تصویری را به یک فضای برداری مشترک تبدیل کند و در کاربردهایی مانند بازیابی چندوجهی (\lr{Multimodal Retrieval}) یا محاسبهٔ شباهت تصویر-متن مورد استفاده قرار گیرد.
% *********************************************************************************************


\section{مدل‌های تبدیل صوت به متن (\lr{Speech-to-Text / Automatic Speech Recognition})}
مدل‌های \lr{Speech-to-Text (ASR)} یا تبدیل صوت به متن، سیستم‌هایی هستند که صوت گفتاری انسان را به متن قابل پردازش توسط کامپیوتر تبدیل می‌کنند.
این مدل‌ها معمولاً از شبکه‌های عصبی پیشرفته مانند \lr{RNN}، \lr{LSTM}، \lr{Transformer} یا معماری‌های ترکیبی استفاده می‌کنند و با تحلیل ویژگی‌های صوتی، محتوا و معنای گفتار را استخراج می‌کنند.
هدف اصلی این مدل‌ها، تبدیل سریع و دقیق گفتار به متن برای کاربردهای متنوع در تعامل انسان و ماشین، تحلیل گفتار و پردازش داده‌های صوتی است.
مدل‌های تبدیل صوت به متن پایهٔ بسیاری از سیستم‌های هوش مصنوعی تعاملی، چندرسانه‌ای و پردازش گفتار هستند.
با ترکیب این مدل‌ها با مدل‌های متن و تصویر، می‌توان سیستم‌های \lr{Multimodal RAG} یا چت‌بات‌های هوشمند چندوجهی ایجاد کرد که توانایی پردازش همزمان متن، تصویر و صوت را دارند.

\subsection{کاربردهای مدل‌های تبدیل صوت به متن}
\begin{itemize}
\item دستیارهای صوتی و چت‌بات‌ها: مانند \lr{Siri}، \lr{Alexa} و \lr{Google Assistant} که فرمان‌های صوتی را به متن تبدیل و پردازش می‌کنند.

\item زیرنویس خودکار و کپشنینگ: تولید خودکار زیرنویس برای ویدیوها، کلاس‌های آنلاین یا محتوای رسانه‌ای.

\item تحلیل و جستجوی صوتی: امکان جستجوی محتوا در میان فایل‌های صوتی و استخراج اطلاعات کلیدی.

\item سیستم‌های چندوجهی (\lr{Multimodal}): ترکیب با مدل‌های تصویر و متن در پروژه‌های \lr{RAG} یا \lr{Chatbot} برای درک چندرسانه‌ای.

\item تبدیل گفتار به متن در محیط‌های صنعتی یا پزشکی: مستندسازی جلسات، یادداشت‌های پزشکی و گزارش‌های صوتی.

\end{itemize}

\subsection{نمونه‌هایی از مدل‌های معروف}

% استفاده از description list
\subsubsection{\lr{Whisper (OpenAI)}}
\begin{description}
\item[توضیح] یک مدل بزرگ و چندزبانه که قادر است گفتار را به متن تبدیل کند، از جمله ترجمهٔ همزمان به زبان‌های دیگر.

\item[مزایا] دقت بالا، پشتیبانی از چند زبان، قابلیت استفاده در پروژه‌های \lr{Multimodal}.

\item[محدودیت‌ها] نیاز به منابع سخت‌افزاری قابل توجه برای \lr{inference} در مدل‌های بزرگ.

\end{description}

\subsubsection{\lr{wav2vec 2.0 (Facebook AI / Meta)}}
\begin{description}
\item[توضیح] مبتنی بر معماری \lr{Transformer} و آموزش خودنظارتی (\lr{self-supervised}) روی داده‌های صوتی بزرگ.

\item[مزایا] نیاز کمتر به داده‌های برچسب‌گذاری‌شده، دقت بالا.
\item[محدودیت‌ها] نیاز به آموزش یا \lr{fine-tuning} روی زبان یا لهجهٔ هدف برای عملکرد بهینه.

\end{description}

\subsubsection{\lr{DeepSpeech (Mozilla)}}
\begin{description}
\item[توضیح] مدل متن‌باز مبتنی بر \lr{RNN} که گفتار را به متن تبدیل می‌کند.

\item[مزایا] متن‌باز، سبک و قابل استفاده در محیط‌های محدود منابع.

\item[محدودیت‌ها] دقت پایین‌تر نسبت به مدل‌های \lr{Transformer} بزرگ، عملکرد محدود در محیط‌های نویزی.

\end{description}

\subsubsection{\lr{Julius (Julius Speech Recognition Engine)}}
\begin{description}
\item[توضیح] موتور \lr{ASR} سبک و متن‌باز با تمرکز روی پردازش سریع و قابل اجرا روی سخت‌افزار محدود.

\item[مزایا] سبک و سریع، مناسب برای کاربردهای تعبیه‌شده (\lr{embedded}).
\item[محدودیت‌ها] قابلیت تشخیص لهجه‌ها و زبان‌های متنوع محدود است.

\end{description}


% *******************************************************************************************


\section{Whisper}
\lr{Whisper} یک مدل پیشرفته و چندزبانه‌ی تبدیل گفتار به متن (\lr{Speech-to-Text / ASR}) است که توسط شرکت \lr{OpenAI} توسعه یافته است.
این مدل برای دریافت ورودی‌های صوتی مانند فایل‌های \lr{.wav} یا \lr{.mp3} طراحی شده و خروجی آن متنی شامل محتوای گفتاری موجود در صدا است.
\lr{Whisper} با استفاده از شبکه‌های عصبی ترنسفورمر (\lr{Transformer-based architecture}) آموزش دیده و روی مجموعه‌داده‌ای عظیم شامل حدود ۶۸۰ هزار ساعت گفتار چندزبانه آموزش دیده است.
هدف این مدل فراهم کردن یک سیستم دقیق، مقیاس‌پذیر و قابل استفاده برای کاربردهای متنوع صوتی است.

\subsection{ویژگی‌ها و قابلیت‌های اصلی \lr{Whisper}}
\lr{Whisper} قادر است گفتار را به متن در بیش از ۹۰ تا ۱۰۰ زبان زنده دنیا تبدیل کند و قابلیت تشخیص خودکار زبان گفتاری را نیز دارد.
این مدل علاوه بر تبدیل گفتار به متن، توانایی ترجمه خودکار گفتار به زبان‌های دیگر، از جمله انگلیسی، را نیز فراهم می‌کند.
برخی از ویژگی‌های اصلی \lr{Whisper} عبارت‌اند از:
\begin{itemize}
\item تبدیل گفتار به متن (\lr{Speech-to-Text}): تبدیل مستقیم گفتار انسان به متن قابل پردازش.

\item تشخیص زبان گفتار (\lr{Language Detection}): شناسایی خودکار زبان ورودی صوتی.

\item ترجمه خودکار گفتار (\lr{Speech Translation}): تبدیل گفتار غیرانگلیسی به زبان انگلیسی یا سایر زبان‌ها.

\item پشتیبانی چندزبانه: بیش از ۹۰ زبان، از جمله انگلیسی، فارسی، عربی، فرانسوی، آلمانی، اسپانیایی، روسی، چینی و ژاپنی.

\item کارکرد آفلاین: نیازی به اینترنت برای پردازش صوت ندارد.

\item معماری \lr{Transformer}: امکان پردازش بهینه دنباله‌های طولانی صوتی و برخورداری از دقت بالا در محیط‌های نویزی.

\item آموزش گسترده: توانایی پردازش گفتار طبیعی و لهجه‌های مختلف به دلیل آموزش روی مجموعه داده‌های عظیم و متنوع.

\end{itemize}

\subsection{نسخه‌ها و مدل‌های \lr{Whisper}}
\lr{Whisper} در نسخه‌های مختلف عرضه شده که هرکدام از نظر سرعت پردازش، دقت و حافظه موردنیاز متفاوت هستند:

% اضافه کردن عنوان (caption) به جدول
\begin{table}[h]
\centering
\caption{مقایسه نسخه‌های مختلف مدل Whisper}
\begin{tabular}{|c|c|c|c|c|}
\hline
نام مدل & سرعت پردازش & دقت & حافظه موردنیاز & مناسب برای \\
\hline
\lr{tiny} & بسیار سریع & پایین‌تر & \lr{\textasciitilde75MB} & سیستم‌های سبک یا \lr{real-time} \\
\hline
\lr{base} & سریع & متوسط & \lr{\textasciitilde142MB} & لپ‌تاپ‌ها یا \lr{inference} سریع \\
\hline
\lr{small} & نسبتاً سریع & خوب & \lr{\textasciitilde462MB} & \lr{GPU} ضعیف یا \lr{CPU} قوی \\
\hline
\lr{medium} & کندتر & بالا & \lr{\textasciitilde1.5GB} & \lr{GPU} متوسط (مثلاً \lr{RTX 2060}) \\
\hline
\lr{large} & سنگین & بسیار بالا & \lr{\textasciitilde2.9GB} & \lr{GPU} قدرتمند (مثل \lr{RTX 3090}) \\
\hline
\end{tabular}
\end{table}

نکات 
مهم:
\begin{itemize}
\item تمام نسخه‌ها متن‌باز (\lr{open source}) هستند و از طریق پکیج رسمی \lr{whisper} قابل استفاده‌اند.

\item برای پردازش سریع روی سیستم‌های ضعیف، مدل‌های \lr{tiny} یا \lr{base} مناسب‌اند.

\item برای پروژه‌هایی که دقت بالا اهمیت دارد، مانند پژوهش‌های علمی یا آرشیو صوتی، مدل‌های \lr{medium} یا \lr{large} توصیه می‌شوند.

\end{itemize}

\subsection{کاربردهای \lr{Whisper}}
\begin{itemize}
\item تولید زیرنویس خودکار و کپشنینگ ویدیوها
\item استفاده در سیستم‌های چندرسانه‌ای و \lr{Multimodal RAG} برای ترکیب صوت، تصویر و متن
\item پیاده‌سازی دستیارهای صوتی و چت‌بات‌های هوشمند
\item تحلیل و جستجوی صوتی در فایل‌ها و جلسات ضبط شده
\item مستندسازی و ثبت اطلاعات پزشکی یا صنعتی از طریق گفتار
\end{itemize}

\subsection{مزایا و محدودیت‌ها}
\subsubsection{مزایا}
\begin{itemize}
\item دقت بالا در تبدیل گفتار به متن حتی در محیط‌های نویزی
\item پشتیبانی از زبان‌ها و لهجه‌های متنوع
\item امکان استفاده مستقیم در پروژه‌های \lr{Multimodal}
\item مدل متن‌باز و در دسترس از طریق مخزن رسمی \lr{OpenAI}
\end{itemize}

\subsubsection{محدودیت‌ها}
\begin{itemize}
\item نیاز به منابع سخت‌افزاری قابل توجه برای \lr{inference} مدل‌های بزرگ
\item زمان پردازش طولانی‌تر نسبت به مدل‌های سبک‌تر در 
فایل‌های صوتی طولانی
\item عملکرد در برخی لهجه‌ها یا محیط‌های بسیار نویزی ممکن است کاهش یابد
\end{itemize}

\lr{Whisper} به‌عنوان یک ابزار چندمنظوره، متن‌باز و دقیق برای تبدیل گفتار به متن و ترجمه صوتی، پایه‌ای قوی برای توسعه سیستم‌های هوش مصنوعی تعاملی، چندرسانه‌ای و چندزبانه فراهم می‌کند و قابلیت ادغام آسان با سایر مدل‌ها و سیستم‌های برداری مانند \lr{Weaviate} یا \lr{OpenCLIP} را دارد.

\subsection{نصب و استفاده از \lr{Whisper}}
برای نصب و استفاده از مدل \lr{Whisper}، ابتدا باید بسته رسمی آن را از مخزن \lr{GitHub} دانلود و نصب کنید.
دستور نصب به صورت زیر است:

\begin{latin}
\begin{lstlisting}
pip install git+https://github.com/openai/whisper.git
\end{lstlisting}
\end{latin}

پس از نصب، می‌توانید مدل را در کد خود بارگذاری کرده و استفاده کنید:

\begin{latin}
\begin{lstlisting}
import whisper
whisper_model = whisper.load_model(WHISPER_MODEL, device=DEVICE)
\end{lstlisting}
\end{latin}

در این کد:
\begin{itemize}
\item \lr{WHISPER\_MODEL} می‌تواند یکی از نسخه‌های مدل باشد: \lr{tiny}، \lr{base}، \lr{small}، \lr{medium} یا \lr{large}
\item \lr{DEVICE} مشخص می‌کند مدل روی \lr{GPU (cuda)} یا \lr{CPU} اجرا شود.

\end{itemize}

این روش امکان استفاده سریع از مدل برای تبدیل صوت به متن، تشخیص زبان گفتار و ترجمه صوتی را فراهم می‌کند.
% ********************************************************************************************


\section{پشتیبانی از زبان فارسی}

مدل‌های تولید بردارهای متنی (\lr{Text Embeddings})، از جمله \lr{CLIP}، \lr{OpenAI Embeddings}، \lr{Sentence-BERT} و سایر مدل‌های مشابه، در مرحله آموزش اولیه خود عمدتاً روی داده‌های متنی به زبان انگلیسی آموزش دیده‌اند.
این تمرکز بر زبان انگلیسی به دلیل دسترسی گسترده به داده‌های متنی و منابع آموزشی بزرگ صورت گرفته است.
به‌دنبال این موضوع، کاربرد این مدل‌ها در زبان‌های دیگر، به‌ویژه زبان‌هایی با منابع کمتر مانند فارسی، با برخی چالش‌ها مواجه است.
محدودیت‌های اصلی عبارتند از:
\begin{itemize}
\item کاهش دقت در درک معانی و روابط بین کلمات و جملات غیرانگلیسی
\item کاهش کیفیت شباهت‌سنجی متنی در فضای \lr{embedding}
\item نیاز به تنظیم مجدد (\lr{fine-tuning}) یا استفاده از مدل‌های چندزبانه برای جبران ضعف‌های زبانی
\end{itemize}

\subsection{استفاده از ترجمه خودکار پیش از استخراج \lr{Embedding}}
یکی از راهکارهای رایج برای بهبود عملکرد مدل‌های \lr{Embedding} محور انگلیسی‌محور روی زبان‌های دیگر، از جمله فارسی، استفاده از ترجمه خودکار متن قبل از استخراج \lr{embedding} است.
در این روش، متن غیرانگلیسی ابتدا با یک مدل ترجمه ماشینی مانند \lr{MarianMT} یا سرویس‌های ابری مانند \lr{Google Translate API} به زبان انگلیسی تبدیل می‌شود.
سپس، متن ترجمه‌شده به مدل \lr{embedding} داده شده و بردار متنی (\lr{vector representation}) آن استخراج می‌گردد.
این روش مزایای مشخصی دارد:
\begin{itemize}
\item امکان استفاده مستقیم از مدل‌های پیشرفته و با کیفیت بالا که تنها روی انگلیسی آموزش دیده‌اند، مانند \lr{CLIP} یا \lr{OpenAI Embeddings}.

\item کاهش نیاز به آموزش مجدد یا \lr{fine-tuning} مدل برای زبان‌های با منابع محدود.

\item پیاده‌سازی ساده و سریع، بدون نیاز به تغییر ساختار مدل اصلی یا استفاده از مدل‌های چندزبانه پیچیده.

\end{itemize}

با این حال، محدودیت‌هایی نیز وجود دارد:
\begin{itemize}
\item کیفیت \lr{embedding} به دقت ترجمه بستگی دارد.
هرگونه خطا یا ابهام در ترجمه می‌تواند بر درک معنایی متن و دقت شباهت‌سنجی \lr{embeddings} تأثیر منفی بگذارد.

\item ترجمه خودکار ممکن است برخی ظرایف زبانی، اصطلاحات محلی یا مفاهیم خاص فرهنگی را به درستی منتقل نکند، که در نتیجه \lr{embedding} نهایی ممکن است دقیق نباشد.

\end{itemize}

به‌طور کلی، این روش یک راه‌حل عملی و سریع برای بهره‌گیری از مدل‌های انگلیسی‌محور روی متون فارسی است و در پروژه‌هایی که کیفیت \lr{embedding} انگلیسی مدل بسیار بالا است، می‌تواند به طور مؤثری عملکرد سیستم را بهبود دهد.

\subsection{روش‌های ترجمه متن در پایتون قبل از استخراج \lr{Embedding}}
برای بهبود عملکرد مدل‌های انگلیسی‌محور روی متون غیرانگلیسی مانند فارسی، می‌توان از روش‌های مختلف ترجمه استفاده کرد.
این روش‌ها شامل کتابخانه‌های سبک آفلاین، مدل‌های پیش‌آموزش‌دیده‌شده و مدل‌های بزرگ زبان (\lr{LLM}) هستند.

% استفاده از description list برای مزایا و معایب
\subsubsection{استفاده از کتابخانه‌های ترجمه آفلاین}
کتابخانه‌های متداول این دسته عبارتند از \lr{argos-translate} و \lr{translate}.

\begin{description}
\item[مزایا]
\begin{itemize}
\item قابلیت اجرا به‌صورت آفلاین و بدون نیاز به اینترنت
\item رایگان و سبک، مناسب برای سیستم‌های محلی و پروژه‌های کوچک
\end{itemize}
\item[معایب و محدودیت‌ها]
\begin{itemize}
\item دقت نسبتاً پایین، به‌ویژه برای جملات پیچیده یا محاوره‌ای
\item احتمال عدم ترجمه صحیح برخی اصطلاحات یا کلمات خاص زبان فارسی
\end{itemize}
\end{description}

نمونه کد با \lr{argos-translate}:
\begin{latin}
\begin{lstlisting}
import argostranslate.package
import argostranslate.translate

# نصب بسته ترجمه
argostranslate.package.install_from_path('en_fa.argosmodel')

from argostranslate.translate import translate
translated_text = translate("این یک تست است", "fa", "en")
print(translated_text)
\end{lstlisting}
\end{latin}

\subsubsection{استفاده از مدل‌های \lr{Translator} پیش‌آموزش‌دیده‌شده}
مدل‌هایی مانند \lr{Helsinki-NLP/opus-mt-fa-en} و \lr{Helsinki-NLP/opus-mt-en-fa} از طریق \lr{Hugging Face} در دسترس هستند.

\begin{description}
\item[مزایا]
\begin{itemize}
\item دقت بالاتر نسبت به کتابخانه‌های سبک
\item امکان استفاده آفلاین پس از دانلود مدل
\end{itemize}
\item[محدودیت‌ها]
\begin{itemize}
\item حجم مدل‌ها معمولاً بیشتر است و نیازمند منابع سخت‌افزاری مناسب (\lr{GPU} یا \lr{CPU} قوی) برای پردازش سریع حجم بالای داده‌ها
\end{itemize}
\end{description}

نمونه کد ساده با \lr{Hugging Face}:
\begin{latin}
\begin{lstlisting}
from transformers import MarianMTModel, MarianTokenizer

model_name = "Helsinki-NLP/opus-mt-fa-en"
tokenizer = MarianTokenizer.from_pretrained(model_name)
model = MarianMTModel.from_pretrained(model_name)

text = "این یک تست است"
batch = tokenizer([text], return_tensors="pt", padding=True)
translated = model.generate(**batch)
translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)
print(translated_text)
\end{lstlisting}
\end{latin}

\subsubsection{استفاده از مدل‌های بزرگ زبان (\lr{LLM})}
مدل‌های آزاد و رایگانی مانند \lr{openai/gpt-oss-20b:free} نیز می‌توانند برای ترجمه متون فارسی به انگلیسی استفاده شوند.

\begin{description}
\item[مزایا]
\begin{itemize}
\item دقت بسیار بالا و نزدیک به ترجمه انسانی
\item توانایی درک جملات پیچیده، اصطلاحات محاوره‌ای و متون چندجمله‌ای
\item امکان انجام ترجمه، خلاصه‌سازی و پردازش پیشرفته در یک مرحله
\end{itemize}
\item[محدودیت‌ها]
\begin{itemize}
\item محدودیت تعداد توکن‌ها و حجم داده‌ها
\item نیاز به اتصال اینترنت
\end{itemize}
\end{description}

به‌طور خلاصه، انتخاب روش ترجمه بستگی به دقت مورد نیاز، منابع سخت‌افزاری و حجم داده‌ها دارد.
روش‌های سبک آفلاین مناسب پروژه‌های کوچک و \lr{real-time} هستند، مدل‌های پیش‌آموزش‌دیده‌شده تعادلی بین دقت و منابع ارائه می‌کنند و مدل‌های \lr{LLM} بالاترین دقت و قابلیت‌های پردازشی پیشرفته را دارند.
% **********************************************************************************************


\section{راه اندازی سیستم}
برای آماده‌سازی محیط و ایجاد یک \lr{Collection} در پایگاه داده برداری \lr{Weaviate}، مراحل زیر به صورت رسمی و گام‌به‌گام انجام می‌شوند:

ابتدا داکر را نصب کرده و با اجرای فایل \lr{docker-compose.yml} پایگاه داده \lr{Weaviate} محلی راه‌اندازی می‌شود.
این مرحله باعث ایجاد یک سرویس برداری محلی می‌شود که برای ذخیره و بازیابی \lr{embedding}های متنی، تصویری و صوتی آماده است.
سپس یک محیط مجازی (\lr{Virtual Environment}) ایجاد کرده و تمام کتابخانه‌های مورد نیاز را با استفاده از فایل \lr{requirements.txt} نصب کنید تا وابستگی‌های پروژه فراهم گردد.
مرحله بعد اجرای فایل \lr{create\_database\_collection.py} است که وظیفه ساخت \lr{Collection} را بر عهده دارد.
عملکرد این فایل به شرح زیر است:
\begin{itemize}
\item اتصال به \lr{Weaviate} محلی برقرار می‌شود.

\item بررسی می‌شود که آیا کالکشنی با نام ``\lr{Multimodal\_Collection}'' قبلاً وجود دارد یا خیر.
در صورت وجود، کالکشن قدیمی حذف می‌گردد.
\item یک \lr{Collection} جدید با همان نام ایجاد می‌شود و شامل چهار ویژگی اصلی است:
\begin{itemize}
\item \lr{modality}: نوع داده، شامل متن (\lr{text})، تصویر (\lr{image}) یا صوت (\lr{audio})
\item \lr{content}: برای داده‌های متنی، خود متن؛
برای داده‌های صوتی، متن تبدیل شده (\lr{transcription})؛ برای تصاویر، رشته خالی ``''
\item \lr{contentId}: شناسه یکتا برای هر محتوا
\item \lr{filePath}: مسیر نسبی فایل روی سیستم
\end{itemize}
\end{itemize}

در پایان، اتصال به \lr{Weaviate} به‌صورت ایمن بسته می‌شود و محیط برای ورود داده‌های \lr{Multimodal} آماده خواهد بود.
این فرآیند تضمین می‌کند که یک \lr{Collection} منظم و قابل استفاده برای ذخیره \lr{embedding}ها و پردازش‌های بعدی در پروژه‌های \lr{Multimodal RAG} آماده گردد.
% ******************************************************************************************


\section{وارد کردن دیتا به دیتابیس}

\subsection{روند پردازش داده‌ها در پروژه}
در این پروژه، فرایند پردازش داده‌ها به شکل زیر سازمان‌دهی شده است:

ابتدا برای تمام فایل‌های صوتی، از مدل \lr{Whisper} استفاده می‌شود تا متن متناظر (\lr{transcription}) هر فایل صوتی استخراج گردد.
به این ترتیب، داده‌های صوتی به داده‌های متنی قابل پردازش تبدیل می‌شوند و یکپارچگی با سایر داده‌های متنی پروژه برقرار می‌گردد.
پس از این مرحله، دو نوع داده اصلی در اختیار داریم:
\begin{itemize}
\item متن (\lr{Text})
\item تصویر (\lr{Image})
\end{itemize}

سپس با استفاده از مدل \lr{CLIP}، هر دو نوع داده متنی و تصویری به بردارهای عددی (\lr{embeddings}) تبدیل می‌شوند.
این بردارها نمایانگر معنای محتوای داده‌ها هستند و امکان اندازه‌گیری شباهت معنایی بین متن‌ها و تصاویر را فراهم می‌کنند.
در نهایت، این \lr{embeddings} همراه با اطلاعات مربوط به هر داده در پایگاه داده برداری \lr{Weaviate} ذخیره می‌شوند.
این کار امکان استفاده از داده‌ها در مراحل بعدی پروژه برای جست‌وجو، بازیابی و پاسخ‌گویی چندوجهی (\lr{Multimodal Retrieval and Generation}) را فراهم می‌سازد.
این فرایند، پایه‌ای قوی برای سیستم‌های چندرسانه‌ای و کاربردهای \lr{RAG} فراهم می‌کند و تضمین می‌کند که داده‌ها به شکل منظم و استاندارد برای پردازش‌های معنایی آماده باشند.

\subsection{روش ترجمه متن‌ها و فایل‌های صوتی در پروژه}
در این پروژه، پیش از ذخیره‌سازی داده‌ها در پایگاه داده و انجام مراحل \lr{retrieval}، متون فارسی با استفاده از مدل \lr{LLM} رایگان \lr{openai/gpt-oss-20b:free} ترجمه می‌شوند.
دلیل انتخاب این مدل، دقت بالای ترجمه و همچنین رایگان بودن آن است که امکان پردازش حجم بالای داده‌ها را فراهم می‌کند.
برای فایل‌های صوتی نیز روند مشابهی اعمال می‌شود. اگر فایل صوتی به زبان انگلیسی باشد، متن استخراج‌شده همان‌طور در دسترس قرار می‌گیرد.
در غیر این صورت، ابتدا با استفاده از مدل \lr{Whisper} متن اصلی استخراج شده و سپس در صورت نیاز به انگلیسی ترجمه می‌شود.
نمونه کد استفاده‌شده در این پروژه به صورت زیر است:

\begin{latin}
\begin{lstlisting}
def transcribe_to_english(path):
    """Transcribe audio to English using Whisper."""
    res = whisper_model.transcribe(path, task="transcribe")
    lang, text = res.get("language", ""), res.get("text", "").strip()
    
    # اگر زبان انگلیسی بود، متن همان‌جا بازگردانده می‌شود
    if lang.startswith("en"):
        return text
    
    # در غیر این صورت، از قابلیت ترجمه Whisper استفاده می‌کنیم
    return (
        whisper_model.transcribe(path, task="translate", language="en")
 
       .get("text", "")
        .strip()
    )
\end{lstlisting}
\end{latin}

در این روش:
\begin{itemize}
\item ابتدا \lr{Transcription} انجام می‌شود تا متن اصلی فایل صوتی شناسایی گردد.

\item اگر زبان فایل انگلیسی باشد، متن استخراج‌شده بازگردانده می‌شود.

\item در صورت غیرانگلیسی بودن فایل، از قابلیت \lr{Translate} مدل \lr{Whisper} برای تبدیل متن به انگلیسی استفاده می‌شود.

\end{itemize}

به این ترتیب، تمامی فایل‌های صوتی به متن انگلیسی تبدیل شده و آماده پردازش با مدل‌هایی مانند \lr{CLIP} و ذخیره در پایگاه داده برداری می‌شوند.
همچنین، در مرحله‌ی پردازش کوئری‌های کاربران، ممکن است متن ورودی یا فایل صوتی به زبان فارسی باشد.
برای امکان جست‌وجوی دقیق در دیتابیس برداری که مبتنی بر متن انگلیسی است، ابتدا کوئری‌ها به انگلیسی ترجمه شده و سپس مورد پردازش و بازیابی اطلاعات قرار می‌گیرند.

\subsection{راهنمای رسمی آماده‌سازی و وارد کردن داده‌ها به پایگاه داده برداری (Weaviate)}
برای آماده‌سازی داده‌ها و وارد کردن آن‌ها به پایگاه داده برداری در این پروژه، مراحل زیر به‌صورت رسمی و مرحله‌ای انجام می‌شوند:

% حذف شماره‌گذاری دستی
\subsubsection{ساختار پوشه‌ها و قرار دادن داده‌ها}

تمام داده‌های پروژه باید در مسیرهای مشخص زیر قرار گیرند:
\begin{itemize}
\item \textbf{تصاویر:} \\
\lr{PROJECT\_ROOT/content/image\_dataset}
\item \textbf{فایل‌های صوتی:} \\
\lr{PROJECT\_ROOT/content/audio\_dataset}
\item \textbf{متون:} ابتدا یک فایل \lr{CSV} با نام \lr{text\_data.csv} ایجاد کرده و دو ستون داشته باشد:
\begin{itemize}
\item \lr{id} برای شناسه‌ی متن
\item \lr{text} برای محتوای متن
\end{itemize}
سپس این فایل در مسیر زیر قرار داده شود: \\
\lr{PROJECT\_ROOT/content/text\_dataset}
\end{itemize}

% حذف شماره‌گذاری دستی
\subsubsection{مدل OpenCLIP و وزن‌های آن}

در این پروژه برای تولید \lr{embedding} از مدل \lr{OpenCLIP} استفاده شده است.
وزن‌های مدل باید پیش از اجرای اسکریپت‌ها دانلود و در مسیر زیر قرار گیرند: \\
\lr{PROJECT\_ROOT/open\_clip\_weights/ViT-B-32-openai/open\_clip\_model.safetensors}

\begin{itemize}
\item مدل مورد استفاده در این پروژه: \lr{ViT-B-32}
\item در صورت داشتن منابع سخت‌افزاری مناسب (\lr{RAM} بالای ۸ گیگ و \lr{GPU})، امکان استفاده از مدل‌های قوی‌تر نیز وجود دارد.

\end{itemize}

همچنین لازم است متغیرهای زیر در فایل پیکربندی \lr{config.py} تنظیم شوند:

\begin{latin}
\begin{lstlisting}
OPENROUTER_API_KEY = os.getenv("LLM_API_KEY")
OPENROUTER_API_BASE = "https://openrouter.ai/api/v1"
LLM_MODEL_NAME = "openai/gpt-oss-20b:free"
\end{lstlisting}
\end{latin}

\begin{itemize}
\item \lr{API Key} در فایل \lr{.env} با کلید \lr{LLM\_API\_KEY} ذخیره می‌شود.

\item مدل رایگان \lr{openai/gpt-oss-20b:free} برای ترجمه متون فارسی به انگلیسی استفاده شده است.

\item در صورت نیاز به دقت بالاتر، می‌توان از مدل‌های قوی‌تر مانند \lr{Gemini 2.5} یا \lr{ChatGPT-4} استفاده کرد، البته با توجه به حجم بالای داده‌ها باید مصرف توکن‌ها کنترل شود.

\end{itemize}

% حذف شماره‌گذاری دستی
\subsubsection{وارد کردن داده‌ها به پایگاه داده}
دو روش برای وارد کردن داده‌ها وجود دارد:

\begin{enumerate}
\item \textbf{Import محلی:} داده‌های محلی روی سیستم شما به \lr{embedding} تبدیل و مستقیماً در پایگاه داده \lr{Weaviate} ذخیره می‌شوند.
\\
مسیر اسکریپت‌ها: \\
\lr{PROJECT\_ROOT/import\_data\_into\_database/import\_local\_data}

\item \textbf{استفاده از سیستم دیگر / Cloud:} داده‌ها در محیط ابری (مثلاً \lr{Google Colab}) به \lr{embedding} تبدیل می‌شوند و ابتدا در پایگاه داده ابری ذخیره می‌شوند.
سپس داده‌ها از آنجا \lr{export} و به صورت \lr{JSON} روی سیستم لوکال ذخیره شده و با اسکریپت وارد پایگاه داده محلی می‌شوند.
\\
مسیر اسکریپت‌ها: \\
\lr{PROJECT\_ROOT/import\_data\_into\_database/} \\
\lr{export\_data\_from\_weaviate\_cloud\_into\_json\_file\_and\_import\_it\_into\_local\_database}
\end{enumerate}

این روش برای سیستم‌های ضعیف و فاقد \lr{GPU} توصیه می‌شود.

% حذف شماره‌گذاری دستی
\subsubsection{روش اول – Import داده‌های محلی}

\textbf{ایمپورت داده‌های متنی:}
\begin{itemize}
\item فایل \lr{CSV} متون (فارسی یا انگلیسی) را در مسیر \lr{PROJECT\_ROOT/content/text\_dataset} قرار دهید.

\item نام فایل را در اسکریپت \lr{import\_text\_dataset\_into\_database.py} تنظیم کرده و اجرا کنید.

\item این اسکریپت مراحل زیر را انجام می‌دهد:
\begin{itemize}
\item خواندن داده‌ها از \lr{CSV}
\item ترجمه متون غیرانگلیسی با استفاده از مدل \lr{LLM} مشخص‌شده در \lr{config.py}
\item تولید \lr{embedding} هر متن با مدل \lr{OpenCLIP}
\item ذخیره \lr{embedding}ها در پایگاه داده \lr{Weaviate}
\end{itemize}
\end{itemize}

\textbf{ایمپورت داده‌های تصویری:}
\begin{itemize}
\item تصاویر را در مسیر \lr{PROJECT\_ROOT/content/image\_dataset} قرار دهید.

\item اجرای اسکریپت: \lr{import\_image\_dataset\_into\_database.py}
\item این اسکریپت برای هر تصویر \lr{embedding} تولید کرده و در دیتابیس ذخیره می‌کند.

\end{itemize}

\textbf{ایمپورت داده‌های صوتی:}
\begin{itemize}
\item فایل‌های صوتی را در مسیر \lr{PROJECT\_ROOT/content/audio\_dataset} قرار دهید.

\item اجرای اسکریپت: \lr{import\_audio\_dataset\_into\_database.py}
\item مراحل اجرا:
\begin{itemize}
\item تشخیص زبان فایل صوتی با مدل \lr{Whisper}
\item ترجمه به انگلیسی در صورت نیاز
\item ذخیره اطلاعات فایل و متن استخراج‌شده در \lr{JSON}
\item تولید \lr{embedding} متن صوتی با \lr{OpenCLIP} و ذخیره در پایگاه داده
\end{itemize}
\end{itemize}

% حذف شماره‌گذاری دستی
\subsubsection{روش دوم – استفاده از سیستم دیگر / Cloud}

\begin{itemize}
\item داده‌ها را در مسیرهای مشخص شده در \lr{Colab} یا سیستم ابری قرار دهید.

\item تمام سلول‌های \lr{Jupyter Notebook} در مسیر زیر به ترتیب اجرا شوند: \\
\lr{PROJECT\_ROOT/import\_data\_into\_database/} \\
\lr{export\_data\_from\_weaviate\_cloud\_into\_json\_file\_and\_import\_it\_into\_local\_database}
\item پس از پایان اجرا، فایل \lr{JSON} یا \lr{CSV} حاصل دانلود و در مسیر مشخص قرار داده شود.

\item نام فایل در اسکریپت \lr{import\_data\_from\_json\_into\_local\_weaviate.py} تنظیم شده و اسکریپت اجرا می‌شود.

\item با این کار تمام داده‌ها به پایگاه داده محلی منتقل می‌شوند.

\item دقت شود که داده‌ها باید در همان پوشه‌های اولیه وجود داشته باشند تا امکان بازیابی (\lr{retrieve}) صحیح فراهم باشد.

\end{itemize}

این فرآیند تضمین می‌کند که داده‌های متنی، تصویری و صوتی به‌صورت منظم وارد پایگاه داده برداری شوند و برای پردازش‌های \lr{Multimodal RAG} آماده باشند.
% *******************************************************************************************


\section{پردازش کوئری‌های کاربر و بازیابی داده‌ها از سیستم}

کاربران می‌توانند هر ترکیبی از داده‌های متن، تصویر یا صوت را به عنوان کوئری به سیستم وارد کنند.
روند پردازش کوئری به صورت یکپارچه و رسمی به شرح زیر است:

% حذف شماره‌گذاری دستی
\subsection{پردازش اولیه کوئری}


% ارتقا به subsubsection
\subsubsection{کوئری متنی}
در صورتی که متن ورودی غیرانگلیسی باشد، ابتدا با استفاده از تابع \lr{normalize\_text\_to\_english} (یا مشابه آن) به انگلیسی ترجمه می‌شود.
برای این کار در این پروژه از کتابخانه‌ی \lr{deep\_translator} استفاده شده است:

\begin{latin}
\begin{lstlisting}
# server/language_utils.py
from deep_translator import GoogleTranslator

def normalize_text_to_english(text: str) -> str:
    """
    Convert any input text into English if it is not already English.
    Works offline for language detection + uses Google Translate API.
    """
    if not text or text.strip() == "":
        return text

    try:
        translated = GoogleTranslator(source='auto', target='en').translate(text)
        return translated
  
  except Exception as e:
        print(f"Translation error: {e}")
        return text
\end{lstlisting}
\end{latin}

کتابخانه \lr{deep\_translator} یک ابزار سبک و سریع برای ترجمه متن است که با منابعی مانند \lr{Google Translate}، \lr{Deepl} و \lr{Pons} کار می‌کند.
مزیت اصلی آن سرعت بالا و مناسب بودن برای کاربردهای \lr{real-time} است، اما دقت آن نسبت به مدل‌های \lr{LLM} کمتر است.

% ارتقا به subsubsection
\subsubsection{کوئری صوتی}
اگر ورودی کاربر یک فایل صوتی باشد، ابتدا متن آن با استفاده از مدل \lr{Whisper} استخراج می‌شود.
سپس در صورت غیرانگلیسی بودن، متن به انگلیسی ترجمه می‌شود.
این کار تضمین می‌کند که تمامی کوئری‌ها، چه متنی و چه صوتی، قبل از تبدیل به \lr{embedding}، به زبان انگلیسی آماده پردازش شوند.

% حذف شماره‌گذاری دستی
\subsection{تبدیل کوئری به \lr{embedding}}
متن نهایی (انگلیسی) یا داده‌های تصویری با استفاده از مدل‌های \lr{CLIP} یا \lr{embedding text model} به بردارهای عددی (\lr{embeddings}) تبدیل می‌شوند.
این بردارها نمایانگر معنای محتوای داده هستند و امکان مقایسه معنایی بین داده‌ها را فراهم می‌کنند.

% حذف شماره‌گذاری دستی
\subsection{بازیابی داده‌های مشابه از پایگاه داده}

\begin{itemize}
\item \lr{embedding} کوئری با \lr{embedding}‌های ذخیره‌شده در \lr{Weaviate} مقایسه می‌شود.

\item با استفاده از متد \lr{collection.query.near\_vector}، \lr{k} مورد نزدیک‌ترین \lr{embedding} انتخاب و داده‌های مرتبط به کاربر نمایش داده می‌شوند.

\item در صورتی که کاربر بیش از یک ورودی داشته باشد، ابتدا \lr{embedding} هر ورودی جداگانه محاسبه می‌شود و سپس با میانگین‌گیری (\lr{average pooling}) ترکیب می‌شوند تا یک \lr{embedding} واحد نماینده کل کوئری ایجاد گردد.
این \lr{embedding} واحد برای جست‌وجو در دیتابیس استفاده می‌شود.
\end{itemize}

% حذف شماره‌گذاری دستی
\subsection{پشتیبانی از کوئری‌های چندمدالیته و چندگانه}

این مکانیزم باعث می‌شود سیستم بتواند:
\begin{itemize}
\item کوئری‌های متنی و صوتی چندگانه
\item کوئری‌های ترکیبی متن، تصویر و صوت
\end{itemize}
را به شکل مؤثر پردازش کرده و نزدیک‌ترین نتایج معنایی را به کاربر ارائه دهد.

\subsection{جمع‌بندی}
با استفاده از روش‌های فوق، سیستم اطمینان حاصل می‌کند که تمام کوئری‌ها به انگلیسی تبدیل شده، \lr{embedding} آن‌ها تولید شده و با پایگاه داده برداری مقایسه می‌شوند تا نتایج \lr{retrieval} سریع و دقیق ارائه گردد.
این رویکرد امکان پردازش \lr{real-time} برای کاربران فارسی‌زبان و چندمدالیته را فراهم می‌آورد و هماهنگی کامل بین متن، تصویر و صوت را تضمین می‌کند.
% ********************************************************************************************


\section{پردازش داده‌های بازیابی شده توسط \lr{LLM}}
پس از مرحله‌ی \lr{Multimodal Retrieval}، داده‌های بازیابی شده برای تحلیل و تولید پاسخ توسط یک \lr{LLM} چندوجهی آماده می‌شوند.
در این پروژه، از مدل \lr{openai/gpt-5-image-mini} استفاده شده و ارتباط با آن از طریق \lr{OpenRouter API} برقرار می‌گردد:

\begin{latin}
\begin{lstlisting}
client = OpenAI(
    base_url=LLM_API_BASE,  # "https://openrouter.ai/api/v1"
    api_key=LLM_API_KEY
)
\end{lstlisting}
\end{latin}

\subsection{داده‌های ارسال‌شده به \lr{LLM}}
داده‌هایی که به \lr{LLM} فرستاده می‌شوند شامل موارد زیر هستند:

\begin{enumerate}
\item \textbf{ورودی‌های کاربر}
\begin{itemize}
\item متن‌های کوئری
\item تصاویر ارسال‌شده
\item فایل‌های صوتی (فقط متن استخراج‌شده از آنها، بدون ارسال فایل خام)
\end{itemize}

\item \textbf{موارد بازیابی‌شده (\lr{retrieved})}
\begin{itemize}
\item یک نمونه متن \lr{retrieved}
\item یک تصویر \lr{retrieved}
\item یک فایل صوتی \lr{retrieved} به صورت متن \lr{transcribe} شده
\end{itemize}
\end{enumerate}

\textbf{نکته:} فایل‌های صوتی خام مستقیماً به مدل ارسال نمی‌شوند و تنها متن استخراج شده از آن‌ها پردازش می‌شود.

\subsection{روند پردازش داخل تابع \lr{feed\_data\_into\_llm}}
\begin{enumerate}
\item داده‌های ورودی و \lr{retrieved} به بخش‌های جداگانه (\lr{sections}) تفکیک می‌شوند: متن، تصویر، صوت.

\item تصاویر به ابعاد کوچک (\lr{128x128}) تغییر اندازه داده شده و به \lr{Base64} تبدیل می‌شوند تا امکان ارسال آن‌ها به \lr{LLM} فراهم گردد.

\item متن‌های صوت و \lr{retrieved} نیز در قالب \lr{Markdown} آماده‌سازی می‌شوند.

\item تمام بخش‌ها با هم ترکیب شده و یک \lr{context} کامل ایجاد می‌شود.

\item این \lr{context} به مدل \lr{LLM} ارسال می‌گردد:

\begin{latin}
\begin{lstlisting}
completion = client.chat.completions.create(
    model=LLM_MODEL_NAME,  # e.g., "openai/gpt-5-image-mini"
    messages=[
        {"role": "system", "content": "You are a multimodal reasoning assistant..."},
        {"role": "user", "content": content_list}
    ]
)
\end{lstlisting}
\end{latin}

\item پاسخ تولیدشده توسط \lr{LLM} در متغیر \lr{response\_text} ذخیره می‌شود.

\end{enumerate}

\subsection{نکات کلیدی}
\begin{itemize}
\item مدل \lr{LLM} قادر است داده‌های چندوجهی (متن، تصویر، صوت) را تحلیل کرده و پاسخ یا توصیف جامع و دقیق تولید کند.

\item تمامی داده‌های ارسالی و \lr{context} مورد استفاده توسط مدل لاگ می‌شوند تا امکان بررسی و بازبینی فراهم گردد.

\item این روش تضمین می‌کند که کاربر یک پاسخ کامل و متناسب با تمام داده‌ها دریافت کند، حتی اگر کوئری و داده‌های بازیابی شامل چندین نوع مودالیتی باشد.

\end{itemize}

این ساختار پردازش، امکان تحلیل دقیق و تولید پاسخ‌های چندوجهی و هماهنگ با داده‌های واقعی را فراهم می‌آورد.
% *********************************************************************************************


\section{نحوه اجرای پروژه}
در این سیستم، سرور با استفاده از \lr{FastAPI} پیاده‌سازی شده و کلاینت (فرانت‌اند) با استفاده از \lr{HTML}، \lr{CSS} و \lr{Vanilla JavaScript} پیاده‌سازی شده است.
برای اجرای پروژه، پس از تکمیل مراحل اولیه راه‌اندازی، مراحل زیر را دنبال کنید:

\begin{enumerate}
\item \textbf{فعال‌سازی محیط مجازی:} محیط مجازی (\lr{virtual environment}) خود را فعال کنید.

\item \textbf{ورود به مسیر پروژه:} در ترمینال به مسیر ریشه پروژه (\lr{PROJECT\_ROOT}) بروید.

\item \textbf{اجرای سرور:} دستور زیر را اجرا کنید:

\begin{latin}
\begin{lstlisting}
python -m server.main
\end{lstlisting}
\end{latin}

\item \textbf{دسترسی به رابط کاربری:} یک مرورگر باز کرده و به آدرس زیر مراجعه کنید:

\begin{latin}
\begin{lstlisting}
http://localhost:8000
\end{lstlisting}
\end{latin}
\end{enumerate}

با انجام این مراحل، سرور راه‌اندازی شده و رابط کاربری پروژه در مرورگر قابل دسترسی خواهد بود.

\end{document}